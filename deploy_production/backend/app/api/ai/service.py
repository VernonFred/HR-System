"""
AI æœåŠ¡å±‚ - å¤šæ¨¡å‹åˆ†å±‚è·¯ç”±ç‰ˆ

è·¯ç”±ç­–ç•¥ï¼š
1. Level Aï¼ˆä¸»åŠ›ï¼‰: Qwen2.5-7B-Instruct - æ—¥å¸¸ç”»åƒåˆ†æ
2. Level Bï¼ˆé«˜é˜¶ï¼‰: Qwen2.5-32B-Instruct - é«˜çº§å²—ä½/ç®¡ç†å²—
3. Level Cï¼ˆä¸“å®¶ï¼‰: DeepSeek-R1-0528 - æ·±åº¦æ´å¯Ÿ/é¢è¯•è¿½é—®

Fallback ç­–ç•¥ï¼š
ModelScope å¤±è´¥ â†’ ç¡…åŸºæµåŠ¨ Qwen3-8B â†’ GLM fallback é“¾
"""

import logging
from functools import lru_cache
from typing import Any, Dict, List

from app.core.ai.ai_client import AIClientError, parse_json_safely, pick_content_text, post_chat
from app.core.ai import prompt_builder
from app.core.ai.portrait_router import (
    call_portrait_model, 
    should_use_pro_level,
    generate_expert_analysis,
    get_router_status,
)

logger = logging.getLogger(__name__)


@lru_cache(maxsize=128)
def _cache_placeholder() -> Dict[str, Dict[str, Any]]:
    # é€šè¿‡ lru_cache è¿”å›åŒä¸€ä¸ª dict å®ä¾‹ï¼Œé¿å…å…¨å±€å˜é‡æ£€æŸ¥å‘Šè­¦
    return {}


def _get_cache() -> Dict[str, Dict[str, Any]]:
    return _cache_placeholder()


def _cache_get(key: str) -> Dict[str, Any] | None:
    return _get_cache().get(key)


def _cache_set(key: str, value: Dict[str, Any]) -> None:
    _get_cache()[key] = value


async def ai_interpretation(
    payload: Dict[str, Any],
    force_pro: bool = False,
    use_expert_summary: bool = False,
) -> Dict[str, Any]:
    """
    AI ç”»åƒè§£è¯» - å¤šæ¨¡å‹åˆ†å±‚è·¯ç”±ç‰ˆ.
    
    è·¯ç”±é€»è¾‘ï¼š
    1. æ£€æŸ¥æ˜¯å¦éœ€è¦ Pro çº§åˆ†æï¼ˆé«˜çº§å²—ä½/ç®¡ç†å²—/æç«¯åˆ†æ•°ï¼‰
    2. ä¼˜å…ˆä½¿ç”¨ ModelScope æ¨¡å‹
    3. ModelScope å¤±è´¥æ—¶ fallback åˆ°ç¡…åŸºæµåŠ¨
    4. å¦‚æœå¯ç”¨ä¸“å®¶çº§ç»¼åˆåˆ†æï¼Œä½¿ç”¨äºŒé˜¶æ®µç”Ÿæˆ
    
    Args:
        payload: ç”»åƒç”Ÿæˆå‚æ•°
        force_pro: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨ Pro çº§åˆ†æ
        use_expert_summary: æ˜¯å¦ä½¿ç”¨ä¸“å®¶çº§ç»¼åˆåˆ†æï¼ˆäºŒé˜¶æ®µç”Ÿæˆï¼Œç”¨ DeepSeek-R1ï¼‰
        
    Returns:
        ç”»åƒç»“æœå­—å…¸
    """
    # ä¸“å®¶åˆ†ææ¨¡å¼ä¸ä½¿ç”¨ç¼“å­˜ï¼ˆæ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆï¼‰
    cache_key = f"interpretation:{payload.get('submission_code')}"
    if payload.get("submission_code") and not use_expert_summary:
        cached = _cache_get(cache_key)
        if cached:
            return cached

    # åˆ¤æ–­åˆ†æçº§åˆ«
    position = payload.get("position_keywords", [""])[0] if payload.get("position_keywords") else ""
    
    # â­ å…³é”®ä¿®å¤ï¼šä¸“å®¶åˆ†æä½¿ç”¨ expert çº§åˆ«ï¼Œå¦åˆ™åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ pro
    if use_expert_summary:
        level = "expert"
        logger.info("ğŸ§  ä½¿ç”¨ä¸“å®¶åˆ†ææ¨¡å¼ (level=expert, DeepSeek-R1)")
    else:
        use_pro = should_use_pro_level(
            position=position,
            force_pro=force_pro,
            competency_scores=payload.get("competency_scores"),
        )
        level = "pro" if use_pro else "normal"
        logger.info(f"ğŸ“Š ä½¿ç”¨åˆ†æçº§åˆ«: {level}")
    
    # â­ å…³é”®ä¿®å¤ï¼šä¼ å…¥ level å‚æ•°ä»¥é€‰æ‹©å¯¹åº”çš„æç¤ºè¯
    # ğŸŸ¢ P2-3å¢å¼º: ä¼ é€’å€™é€‰å²—ä½å‚è€ƒç»™æç¤ºè¯æ„å»ºå™¨
    candidate_positions = payload.get("candidate_positions")
    messages = prompt_builder.build_interpretation_prompt(
        payload, 
        level=level,
        candidate_positions=candidate_positions  # ğŸŸ¢ ä¼ é€’å€™é€‰å²—ä½
    )
    
    try:
        # ä½¿ç”¨ç”»åƒä¸“ç”¨è·¯ç”±å™¨
        resp = await call_portrait_model(
            messages=messages,
            level=level,
            max_tokens=1536,
            temperature=0.3,
        )
        data = parse_json_safely(pick_content_text(resp))
        data = _fill_interpretation_defaults(data)
        
        # è®°å½•ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
        data["_model"] = resp.get("model", "unknown")
        data["_level"] = resp.get("level", level)
        
        logger.info(f"âœ… AIç”»åƒç”ŸæˆæˆåŠŸ model={data.get('_model')} level={data.get('_level')}")
        
        # äºŒé˜¶æ®µç”Ÿæˆï¼šä¸“å®¶çº§ç»¼åˆåˆ†æ
        if use_expert_summary:
            logger.info("ğŸ§  å¯ç”¨äºŒé˜¶æ®µç”Ÿæˆï¼šä¸“å®¶çº§ç»¼åˆåˆ†æ")
            from app.core.ai.portrait_router import generate_expert_summary
            
            scores = payload.get("scores", {})
            job_family = payload.get("job_family", "é€šç”¨")
            
            expert_result = await generate_expert_summary(
                basic_portrait=data,
                scores=scores,
                target_position=position,
                job_family=job_family,
            )
            
            # åˆå¹¶ä¸“å®¶åˆ†æç»“æœ
            if expert_result.get("expert_summary"):
                data["summary_points"] = expert_result.get("expert_summary", [])
                data["hiring_recommendation"] = expert_result.get("hiring_recommendation", "")
                data["interview_focus"] = expert_result.get("interview_focus", [])
                data["_expert_model"] = expert_result.get("_model", "unknown")
                logger.info("âœ… ä¸“å®¶çº§ç»¼åˆåˆ†æå·²åˆå¹¶")
        
    except Exception as exc:  # noqa: BLE001
        logger.warning("ai_interpretation failed, return fallback: %s", exc)
        data = _fill_interpretation_defaults({})
        data["_error"] = str(exc)

    if payload.get("submission_code") and not use_expert_summary:
        _cache_set(cache_key, data)
    return data


async def ai_expert_analysis(
    summary_json: Dict[str, Any],
    scores: Dict[str, Any],
    job_family: str,
    target_position: str,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆä¸“å®¶çº§æ·±åº¦åˆ†æ.
    
    ä½¿ç”¨ DeepSeek-R1 å¯¹å·²æœ‰çš„ç”»åƒæ‘˜è¦è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œ
    è¾“å‡º 3 æ¡æ·±åº¦æ´å¯Ÿæˆ–é¢è¯•è¿½é—®å»ºè®®ã€‚
    
    Args:
        summary_json: 7B/32B ç”Ÿæˆçš„ç»“æ„åŒ–æ‘˜è¦
        scores: æµ‹è¯„åˆ†æ•°
        job_family: å²—ä½æ—
        target_position: ç›®æ ‡å²—ä½
        
    Returns:
        ä¸“å®¶åˆ†æç»“æœ
    """
    return await generate_expert_analysis(
        summary_json=summary_json,
        scores=scores,
        job_family=job_family,
        target_position=target_position,
    )


def get_ai_router_status() -> Dict[str, Any]:
    """è·å– AI è·¯ç”±å™¨çŠ¶æ€."""
    return get_router_status()


async def ai_match(payload: Dict[str, Any]) -> Dict[str, Any]:
    cache_key = f"match:{payload.get('submission_code')}"
    if payload.get("submission_code"):
        cached = _cache_get(cache_key)
        if cached:
            return cached

    messages = prompt_builder.build_match_prompt(payload)
    try:
        resp = await post_chat(messages)
        data = parse_json_safely(pick_content_text(resp))
        data = _fill_match_defaults(data)
    except Exception as exc:  # noqa: BLE001
        logger.warning("ai_match failed, return fallback: %s", exc)
        data = _fill_match_defaults({})

    if payload.get("submission_code"):
        _cache_set(cache_key, data)
    return data


async def ai_report(payload: Dict[str, Any]) -> Dict[str, Any]:
    cache_key = f"report:{payload.get('submission_code')}"
    if payload.get("submission_code"):
        cached = _cache_get(cache_key)
        if cached:
            return cached

    messages = prompt_builder.build_report_prompt(payload)
    try:
        resp = await post_chat(messages, max_tokens=4096, temperature=0.4)
        data = parse_json_safely(pick_content_text(resp))
        data = _fill_report_defaults(data)
    except Exception as exc:  # noqa: BLE001
        logger.warning("ai_report failed, return fallback: %s", exc)
        data = _fill_report_defaults({})

    if payload.get("submission_code"):
        _cache_set(cache_key, data)
    return data


def _split_summary_to_points(summary: str, target_count: int = 3) -> List[str]:
    """æ™ºèƒ½æ‹†åˆ†summaryä¸ºå¤šæ¡è§‚ç‚¹."""
    if not summary:
        return []
    
    # å…ˆå°è¯•æŒ‰æ®µè½æ‹†åˆ†ï¼ˆ\n\nï¼‰
    paragraphs = [p.strip() for p in summary.split("\n\n") if p.strip()]
    if len(paragraphs) >= target_count:
        return paragraphs[:target_count]
    
    # å¦‚æœæ®µè½ä¸è¶³ï¼Œå°è¯•æŒ‰å¥å­æ‹†åˆ†ï¼ˆã€‚ï¼‰
    sentences = []
    for para in paragraphs:
        # æŒ‰ä¸­æ–‡å¥å·æ‹†åˆ†ï¼Œå¹¶è¿‡æ»¤ç©ºå¥å­
        para_sentences = [s.strip() + "ã€‚" for s in para.split("ã€‚") if s.strip()]
        sentences.extend(para_sentences)
    
    # å¦‚æœå¥å­æ•°é‡è¶³å¤Ÿï¼Œé€‰æ‹©å‰Nå¥
    if len(sentences) >= target_count:
        return sentences[:target_count]
    
    # å¦‚æœè¿˜æ˜¯ä¸å¤Ÿï¼Œè¿”å›åŸæ®µè½
    return paragraphs if paragraphs else [summary]


def _fill_interpretation_defaults(data: Dict[str, Any]) -> Dict[str, Any]:
    """å¡«å…… AI è§£è¯»é»˜è®¤å€¼ - å¢å¼ºç‰ˆ"""
    # å¤„ç†summary_pointsï¼šä¼˜å…ˆä½¿ç”¨AIè¿”å›çš„ï¼Œå¦åˆ™æ™ºèƒ½æ‹†åˆ†summary
    summary_points = data.get("summary_points", [])
    if not summary_points and data.get("summary"):
        summary_points = _split_summary_to_points(data.get("summary", ""), target_count=3)
    
    return {
        "personality_dimensions": data.get("personality_dimensions") or data.get("dimensions") or [],
        "dimensions": data.get("dimensions") or data.get("personality_dimensions") or [],  # å…¼å®¹
        "competencies": data.get("competencies") or [],
        "strengths": data.get("strengths") or [],
        "risks": data.get("risks") or [],
        "summary": data.get("summary") or "",
        "summary_points": summary_points,  # æ–°å¢ï¼š3æ¡æ ¸å¿ƒè§‚ç‚¹
        "quick_tags": data.get("quick_tags") or [],  # â­ æ–°å¢ï¼šå¤´éƒ¨å¿«é€Ÿæ ‡ç­¾
        "suitable_positions": data.get("suitable_positions") or [],
        "unsuitable_positions": data.get("unsuitable_positions") or [],
        "development_suggestions": data.get("development_suggestions") or [],
        "interview_focus": data.get("interview_focus") or [],
    }


def _fill_match_defaults(data: Dict[str, Any]) -> Dict[str, Any]:
    """å¡«å…… AI åŒ¹é…é»˜è®¤å€¼ï¼Œç¡®ä¿è¿”å›åˆ—è¡¨ç±»å‹."""
    def _ensure_list(val):
        if val is None:
            return []
        if isinstance(val, list):
            return val
        if isinstance(val, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ŒåŒ…è£…æˆå•å…ƒç´ åˆ—è¡¨
            return [val] if val.strip() else []
        return []
    
    return {
        "match_analysis": _ensure_list(data.get("match_analysis")),
        "risks": _ensure_list(data.get("risks")),
        "follow_up_questions": _ensure_list(data.get("follow_up_questions")),
    }


def _fill_report_defaults(data: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "markdown": data.get("markdown") or "AI æš‚ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
    }
