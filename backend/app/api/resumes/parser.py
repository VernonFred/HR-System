"""ç®€å†ç®¡ç† - AIè§£ææœåŠ¡."""
import re
import json
import logging
from typing import Dict, List, Any, Optional
from app.api.resumes.schemas import ResumeParsedData, EducationItem, ExperienceItem, ProjectItem

logger = logging.getLogger(__name__)


async def parse_resume_with_ai(
    resume_text: str, 
    analysis_level: str = "pro"
) -> ResumeParsedData:
    """
    ä½¿ç”¨AIè§£æç®€å†æ–‡æœ¬ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯å¹¶è¿›è¡Œæ·±åº¦åˆ†æ.
    
    Args:
        resume_text: ç®€å†æ–‡æœ¬å†…å®¹
        analysis_level: åˆ†æçº§åˆ« (pro/expert)
        
    Returns:
        è§£æåçš„ç»“æ„åŒ–æ•°æ®ï¼ˆåŒ…å«AIåˆ†æç»“æœï¼‰
    """
    from app.core.ai.portrait_router import call_portrait_model
    from app.core.ai.ai_client import parse_json_safely, pick_content_text
    
    # æ„å»ºç®€å†è§£æçš„æç¤ºè¯
    system_prompt = _build_resume_parse_system_prompt()
    user_prompt = _build_resume_parse_user_prompt(resume_text)
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    try:
        logger.info(f"ğŸ” å¼€å§‹AIè§£æç®€å† (level={analysis_level})")
        print(f"ğŸ” å¼€å§‹AIè§£æç®€å† (level={analysis_level})")
        
        # è°ƒç”¨AIæ¨¡å‹
        response = await call_portrait_model(
            messages=messages,
            level=analysis_level,
            max_tokens=2048,
            temperature=0.3,
        )
        
        # è§£æAIè¿”å›çš„JSON
        content = pick_content_text(response)
        result = parse_json_safely(content)
        
        if not result:
            logger.warning("âš ï¸ AIè§£æè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨è§„åˆ™è§£æå…œåº•")
            return _rule_based_parse(resume_text)
        
        logger.info(f"âœ… AIè§£æç®€å†æˆåŠŸ model={response.get('model', 'unknown')}")
        print(f"âœ… AIè§£æç®€å†æˆåŠŸ model={response.get('model', 'unknown')}")
    
        # è½¬æ¢ä¸º ResumeParsedData
        return _convert_ai_result_to_parsed_data(result, resume_text)
        
    except Exception as e:
        logger.warning(f"âŒ AIè§£æç®€å†å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™è§£æå…œåº•")
        print(f"âŒ AIè§£æç®€å†å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™è§£æå…œåº•")
        return _rule_based_parse(resume_text)


def _build_resume_parse_system_prompt() -> str:
    """æ„å»ºç®€å†è§£æçš„ç³»ç»Ÿæç¤ºè¯ - V38å¢å¼ºç‰ˆ."""
    return """ä½ æ˜¯ä¸€åèµ„æ·±çš„äººåŠ›èµ„æºä¸“å®¶å’Œç®€å†åˆ†æå¸ˆï¼Œå…·å¤‡ä¸°å¯Œçš„æ‹›è˜ç»éªŒå’Œäººæ‰è¯„ä¼°èƒ½åŠ›ã€‚

ã€ä½ çš„ä»»åŠ¡ã€‘
1. ç²¾å‡†æå–ç®€å†ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯
2. æ·±åº¦åˆ†æå€™é€‰äººçš„èŒä¸šç‰¹å¾å’Œå‘å±•æ½œåŠ›
3. è¯†åˆ«ç®€å†ä¸­çš„äº®ç‚¹ã€é£é™©å’Œéœ€éªŒè¯çš„ç–‘ç‚¹
4. ä¸ºåç»­çš„äººæ‰ç”»åƒç”Ÿæˆæä¾›é«˜è´¨é‡çš„åˆ†æåŸºç¡€

ã€è¾“å‡ºè¦æ±‚ã€‘
å¿…é¡»è¾“å‡ºåˆæ³• JSONï¼Œç»“æ„å¦‚ä¸‹ï¼š

{
  "name": "å€™é€‰äººå§“å",
  "email": "é‚®ç®±åœ°å€",
  "phone": "æ‰‹æœºå·ç ",
  "location": "æ‰€åœ¨åŸå¸‚",
  "target_position": "æ±‚èŒæ„å‘/ç›®æ ‡å²—ä½",
  "education": [
    {
      "school": "å­¦æ ¡åç§°",
      "major": "ä¸“ä¸š",
      "degree": "å­¦å†ï¼ˆæœ¬ç§‘/ç¡•å£«/åšå£«ï¼‰",
      "start_date": "å¼€å§‹æ—¶é—´",
      "end_date": "ç»“æŸæ—¶é—´"
    }
  ],
  "experience": [
    {
      "company": "å…¬å¸åç§°",
      "position": "èŒä½",
      "start_date": "å¼€å§‹æ—¶é—´",
      "end_date": "ç»“æŸæ—¶é—´",
      "responsibilities": ["èŒè´£æè¿°1", "èŒè´£æè¿°2"],
      "achievements": ["å…·ä½“æˆæœ/ä¸šç»©ï¼ˆå¦‚æœ‰ï¼‰"]
    }
  ],
  "projects": [
    {
      "name": "é¡¹ç›®åç§°",
      "role": "æ‹…ä»»è§’è‰²",
      "start_date": "å¼€å§‹æ—¶é—´",
      "end_date": "ç»“æŸæ—¶é—´",
      "description": "é¡¹ç›®æè¿°",
      "technologies": ["æŠ€æœ¯1", "æŠ€æœ¯2"],
      "impact": "é¡¹ç›®æˆæ•ˆ/å½±å“ï¼ˆå¦‚æœ‰ï¼‰"
    }
  ],
  "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
  "certificates": ["è¯ä¹¦1", "è¯ä¹¦2"],
  "languages": ["è¯­è¨€èƒ½åŠ›1", "è¯­è¨€èƒ½åŠ›2"],
  "summary": "ç®€å†æ‘˜è¦ï¼ˆ50-100å­—ï¼Œæ¦‚æ‹¬å€™é€‰äººçš„æ ¸å¿ƒç«äº‰åŠ›ï¼‰",
  "ai_analysis": {
    "core_strengths": [
      "æ ¸å¿ƒä¼˜åŠ¿1ï¼ˆä»ç»å†ä¸­æ¨æ–­ï¼Œéç®€å†è‡ªè¿°ï¼‰",
      "æ ¸å¿ƒä¼˜åŠ¿2",
      "æ ¸å¿ƒä¼˜åŠ¿3"
    ],
    "potential_risks": [
      "æ½œåœ¨é£é™©1ï¼ˆä»ç»å†ä¸­å‘ç°çš„é—®é¢˜ç‚¹ï¼‰",
      "æ½œåœ¨é£é™©2"
    ],
    "career_trajectory": "èŒä¸šè½¨è¿¹åˆ†æï¼ˆ100-150å­—ï¼‰ï¼šåˆ†æè·³æ§½é¢‘ç‡ï¼ˆæ˜¯å¦é¢‘ç¹/ç¨³å®šï¼‰ã€æ™‹å‡é€Ÿåº¦ï¼ˆæ˜¯å¦æœ‰æ˜æ˜¾ä¸Šå‡ï¼‰ã€è¡Œä¸šé€‰æ‹©ï¼ˆæ˜¯å¦ä¸“æ³¨/è·¨ç•Œï¼‰ã€èŒä¸šå‘å±•è§„å¾‹",
    "work_style": "å·¥ä½œé£æ ¼æ¨æ–­ï¼ˆ80-120å­—ï¼‰ï¼šåŸºäºå·¥ä½œå†…å®¹æ¨æ–­å…¶åšäº‹é£æ ¼ï¼Œå¦‚åæ‰§è¡Œè¿˜æ˜¯åç­–ç•¥ã€åç‹¬ç«‹è¿˜æ˜¯ååä½œã€ååˆ›æ–°è¿˜æ˜¯åç¨³å¥",
    "suitable_environment": "é€‚åˆçš„å·¥ä½œç¯å¢ƒï¼ˆ80-120å­—ï¼‰ï¼šæ¨æ–­é€‚åˆä»€ä¹ˆç±»å‹çš„å›¢é˜Ÿï¼ˆå¤§å‚/åˆ›ä¸šå…¬å¸/ä¼ ç»Ÿä¼ä¸šï¼‰ã€ç®¡ç†é£æ ¼ï¼ˆæ‰å¹³/å±‚çº§ï¼‰ã€å·¥ä½œèŠ‚å¥ï¼ˆå¿«èŠ‚å¥/ç¨³å®šï¼‰",
    "stability_assessment": "ç¨³å®šæ€§è¯„ä¼°ï¼ˆ50-80å­—ï¼‰ï¼šåŸºäºå·¥ä½œæ—¶é•¿ã€è·³æ§½è§„å¾‹æ¨æ–­å€™é€‰äººçš„ç¨³å®šæ€§ï¼Œæ˜¯å¦æœ‰é¢‘ç¹è·³æ§½é£é™©",
    "growth_potential": "æˆé•¿æ½œåŠ›è¯„ä¼°ï¼ˆ80-120å­—ï¼‰ï¼šåŸºäºå­¦å†ã€ç»å†ã€æŠ€èƒ½å‘å±•æ¨æ–­å€™é€‰äººçš„å­¦ä¹ èƒ½åŠ›å’Œæˆé•¿ç©ºé—´",
    "soft_skills_inference": [
      "æ¨æ–­çš„è½¯æŠ€èƒ½1ï¼ˆå¦‚ï¼šæ²Ÿé€šèƒ½åŠ›å¼º - å› ä¸ºæœ‰è·¨éƒ¨é—¨åè°ƒç»å†ï¼‰",
      "æ¨æ–­çš„è½¯æŠ€èƒ½2ï¼ˆå¦‚ï¼šæŠ—å‹èƒ½åŠ›å¥½ - å› ä¸ºæœ‰é«˜å‹é¡¹ç›®ç»å†ï¼‰",
      "æ¨æ–­çš„è½¯æŠ€èƒ½3"
    ],
    "interview_focus_points": [
      "é¢è¯•éœ€é‡ç‚¹éªŒè¯çš„é—®é¢˜1ï¼ˆå¦‚ï¼šæŸæ®µç»å†æ—¶é—´çŸ­ï¼Œéœ€äº†è§£ç¦»èŒåŸå› ï¼‰",
      "é¢è¯•éœ€é‡ç‚¹éªŒè¯çš„é—®é¢˜2ï¼ˆå¦‚ï¼šé¡¹ç›®æˆæœæè¿°æ¨¡ç³Šï¼Œéœ€è¿½é—®å…·ä½“è´¡çŒ®ï¼‰",
      "é¢è¯•éœ€é‡ç‚¹éªŒè¯çš„é—®é¢˜3"
    ],
    "red_flags": [
      "ç®€å†ç–‘ç‚¹/çº¢æ——1ï¼ˆå¦‚ï¼šå·¥ä½œç»å†æœ‰ç©ºç™½æœŸï¼‰",
      "ç®€å†ç–‘ç‚¹/çº¢æ——2ï¼ˆå¦‚ï¼šèŒä½æè¿°ä¸å…¬å¸è§„æ¨¡ä¸åŒ¹é…ï¼‰"
    ],
    "overall_impression": "æ•´ä½“å°è±¡ï¼ˆ100-150å­—ï¼‰ï¼šç»¼åˆè¯„ä»·å€™é€‰äººçš„æ•´ä½“ç´ è´¨ã€ä¸ç›®æ ‡å²—ä½çš„åŒ¹é…åº¦ã€å€¼å¾—å…³æ³¨çš„ç‰¹ç‚¹"
  }
}

ã€åˆ†æåŸåˆ™ã€‘
1. ç‹¬ç«‹åˆ¤æ–­ï¼šä¸è¦è½»ä¿¡ç®€å†ä¸­çš„è‡ªæˆ‘è¯„ä»·ï¼ˆå¦‚"æ²Ÿé€šèƒ½åŠ›å¼º"ï¼‰ï¼Œè¦ä»å…·ä½“ç»å†ä¸­æ¨æ–­
2. æœ‰ç†æœ‰æ®ï¼šæ¯ä¸ªåˆ†æç»“è®ºéƒ½è¦æœ‰ç®€å†å†…å®¹æ”¯æ’‘ï¼Œä¸èƒ½å‡­ç©ºè‡†æ–­
3. å…³æ³¨ç»†èŠ‚ï¼šæ³¨æ„æ—¶é—´çº¿çš„è¿è´¯æ€§ã€èŒä½çš„åˆç†æ€§ã€æˆæœçš„å…·ä½“æ€§
4. å®¢è§‚ä¸­ç«‹ï¼šæ—¢è¦å‘ç°äº®ç‚¹ï¼Œä¹Ÿè¦è¯†åˆ«é£é™©ï¼Œä¿æŒå®¢è§‚
5. å®ç”¨å¯¼å‘ï¼šåˆ†æç»“æœè¦å¯¹æ‹›è˜å†³ç­–æœ‰å®é™…å¸®åŠ©

ã€æ³¨æ„äº‹é¡¹ã€‘
1. å¦‚æœæŸä¸ªå­—æ®µåœ¨ç®€å†ä¸­æ‰¾ä¸åˆ°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²æˆ–ç©ºæ•°ç»„
2. æ—¶é—´æ ¼å¼ç»Ÿä¸€ä¸º "YYYY" æˆ– "YYYY-MM"
3. ai_analysis æ˜¯æ·±åº¦åˆ†æï¼Œå¿…é¡»æœ‰æ´å¯ŸåŠ›ï¼Œç¦æ­¢ç®€å•å¤è¿°ç®€å†å†…å®¹
4. red_flags å’Œ interview_focus_points ç‰¹åˆ«é‡è¦ï¼Œå¸®åŠ©é¢è¯•å®˜å‘ç°éœ€è¦è¿½é—®çš„ç‚¹"""


def _build_resume_parse_user_prompt(resume_text: str) -> str:
    """æ„å»ºç®€å†è§£æçš„ç”¨æˆ·æç¤ºè¯."""
    return f"""è¯·åˆ†æä»¥ä¸‹ç®€å†å†…å®¹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯å¹¶è¿›è¡Œæ·±åº¦åˆ†æï¼š

---ç®€å†å†…å®¹å¼€å§‹---
{resume_text}
---ç®€å†å†…å®¹ç»“æŸ---

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚"""


def _convert_ai_result_to_parsed_data(result: Dict[str, Any], resume_text: str) -> ResumeParsedData:
    """å°†AIè¿”å›çš„ç»“æœè½¬æ¢ä¸º ResumeParsedData - V38å¢å¼ºç‰ˆ."""
    # æå–æ•™è‚²èƒŒæ™¯
    education = []
    for edu in result.get("education", []):
        if isinstance(edu, dict):
            education.append(EducationItem(
                school=edu.get("school", ""),
                major=edu.get("major"),
                degree=edu.get("degree"),
                start_date=edu.get("start_date"),
                end_date=edu.get("end_date")
            ))
    
    # æå–å·¥ä½œç»å†ï¼ˆV38: å¢åŠ  achievementsï¼‰
    experience = []
    for exp in result.get("experience", []):
        if isinstance(exp, dict):
            exp_item = ExperienceItem(
                company=exp.get("company", ""),
                position=exp.get("position", ""),
                start_date=exp.get("start_date"),
                end_date=exp.get("end_date"),
                responsibilities=exp.get("responsibilities", [])
            )
            # å¦‚æœæœ‰æˆå°±ï¼Œæ·»åŠ åˆ°èŒè´£åé¢
            achievements = exp.get("achievements", [])
            if achievements:
                exp_item.responsibilities.extend([f"[æˆæœ] {a}" for a in achievements])
            experience.append(exp_item)
    
    # æå–é¡¹ç›®ç»éªŒï¼ˆV38: å¢åŠ  impactï¼‰
    projects = []
    for proj in result.get("projects", []):
        if isinstance(proj, dict):
            desc = proj.get("description", "")
            impact = proj.get("impact", "")
            if impact:
                desc = f"{desc} | æˆæ•ˆ: {impact}" if desc else impact
            projects.append(ProjectItem(
                name=proj.get("name", ""),
                role=proj.get("role"),
                start_date=proj.get("start_date"),
                end_date=proj.get("end_date"),
                description=desc,
                technologies=proj.get("technologies", [])
            ))
    
    # V38: æ„å»ºæ›´ä¸°å¯Œçš„æ‘˜è¦ï¼ˆåŒ…å«æ‰€æœ‰AIåˆ†æç»´åº¦ï¼‰
    ai_analysis = result.get("ai_analysis", {})
    summary = result.get("summary", "")
    
    # å¦‚æœæœ‰AIåˆ†æï¼Œå°†å…¶æ•´åˆåˆ°æ‘˜è¦ä¸­
    if ai_analysis:
        analysis_parts = []
        
        # æ•´ä½“å°è±¡ï¼ˆæœ€é‡è¦ï¼Œæ”¾æœ€å‰é¢ï¼‰
        if ai_analysis.get("overall_impression"):
            analysis_parts.append(f"ã€æ•´ä½“å°è±¡ã€‘{ai_analysis['overall_impression']}")
        
        # æ ¸å¿ƒä¼˜åŠ¿
        if ai_analysis.get("core_strengths"):
            strengths = ai_analysis['core_strengths']
            if isinstance(strengths, list):
                analysis_parts.append(f"ã€æ ¸å¿ƒä¼˜åŠ¿ã€‘{'ã€'.join(strengths)}")
    
        # æ½œåœ¨é£é™©
        if ai_analysis.get("potential_risks"):
            risks = ai_analysis['potential_risks']
            if isinstance(risks, list):
                analysis_parts.append(f"ã€æ½œåœ¨é£é™©ã€‘{'ã€'.join(risks)}")
        
        # èŒä¸šè½¨è¿¹
        if ai_analysis.get("career_trajectory"):
            analysis_parts.append(f"ã€èŒä¸šè½¨è¿¹ã€‘{ai_analysis['career_trajectory']}")
    
        # å·¥ä½œé£æ ¼
        if ai_analysis.get("work_style"):
            analysis_parts.append(f"ã€å·¥ä½œé£æ ¼ã€‘{ai_analysis['work_style']}")
        
        # é€‚åˆç¯å¢ƒ
        if ai_analysis.get("suitable_environment"):
            analysis_parts.append(f"ã€é€‚åˆç¯å¢ƒã€‘{ai_analysis['suitable_environment']}")
        
        # V38æ–°å¢: ç¨³å®šæ€§è¯„ä¼°
        if ai_analysis.get("stability_assessment"):
            analysis_parts.append(f"ã€ç¨³å®šæ€§è¯„ä¼°ã€‘{ai_analysis['stability_assessment']}")
        
        # V38æ–°å¢: æˆé•¿æ½œåŠ›
        if ai_analysis.get("growth_potential"):
            analysis_parts.append(f"ã€æˆé•¿æ½œåŠ›ã€‘{ai_analysis['growth_potential']}")
        
        # V38æ–°å¢: è½¯æŠ€èƒ½æ¨æ–­
        if ai_analysis.get("soft_skills_inference"):
            skills = ai_analysis['soft_skills_inference']
            if isinstance(skills, list):
                analysis_parts.append(f"ã€è½¯æŠ€èƒ½æ¨æ–­ã€‘{'ï¼›'.join(skills)}")
        
        # V38æ–°å¢: é¢è¯•é‡ç‚¹
        if ai_analysis.get("interview_focus_points"):
            points = ai_analysis['interview_focus_points']
            if isinstance(points, list) and points:
                analysis_parts.append(f"ã€é¢è¯•é‡ç‚¹ã€‘{'ï¼›'.join(points)}")
        
        # V38æ–°å¢: çº¢æ——/ç–‘ç‚¹
        if ai_analysis.get("red_flags"):
            flags = ai_analysis['red_flags']
            if isinstance(flags, list) and flags:
                analysis_parts.append(f"ã€éœ€å…³æ³¨ã€‘{'ï¼›'.join(flags)}")
        
        if analysis_parts:
            summary = summary + "\n\n" + "\n".join(analysis_parts) if summary else "\n".join(analysis_parts)
    
    return ResumeParsedData(
        name=result.get("name", _extract_name_fallback(resume_text)),
        email=result.get("email", _extract_email_fallback(resume_text)),
        phone=result.get("phone", _extract_phone_fallback(resume_text)),
        location=result.get("location", ""),
        target_position=result.get("target_position", ""),
        education=education,
        experience=experience,
        projects=projects,
        skills=result.get("skills", []),
        certificates=result.get("certificates", []),
        languages=result.get("languages", []),
        summary=summary
    )


# =============================================================================
# è§„åˆ™è§£æå…œåº•ï¼ˆå½“AIå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
# =============================================================================

def _rule_based_parse(text: str) -> ResumeParsedData:
    """
    è§„åˆ™è§£æå…œåº•ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å’Œè§„åˆ™è¿›è¡Œç®€å•è§£æ.
    å½“AIè§£æå¤±è´¥æ—¶ä½¿ç”¨ã€‚
    """
    lines = text.split('\n')
    
    return ResumeParsedData(
        name=_extract_name_fallback(lines, text),
        email=_extract_email_fallback(text),
        phone=_extract_phone_fallback(text),
        location=_extract_location_fallback(text),
        target_position=_extract_target_position_fallback(text, lines),
        education=_extract_education_fallback(text),
        experience=_extract_experience_fallback(text),
        projects=[],
        skills=_extract_skills_fallback(text),
        certificates=[],
        languages=[],
        summary="ï¼ˆAIè§£æå¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™æå–åŸºæœ¬ä¿¡æ¯ï¼‰"
    )


def _extract_name_fallback(lines_or_text, text: str = None) -> str:
    """æå–å§“åï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    if isinstance(lines_or_text, str):
        text = lines_or_text
        lines = text.split('\n')
    else:
        lines = lines_or_text
        if text is None:
            text = '\n'.join(lines)
    
    # æ–¹æ³•1ï¼šæŸ¥æ‰¾"å§“åï¼š"åé¢çš„å†…å®¹
    name_patterns = [
        r'å§“\s*å[ï¼š:]\s*([\u4e00-\u9fa5]{2,4})',
        r'Name[ï¼š:]\s*([\u4e00-\u9fa5]{2,4})',
    ]
    for pattern in name_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    # æ–¹æ³•2ï¼šç®€å†å¼€å¤´çš„ä¸­æ–‡å
    for line in lines[:10]:
        line = line.strip()
        if line in ["ä¸ªäººç®€å†", "ç®€å†", "æ±‚èŒç®€å†", "åŸºæœ¬ä¿¡æ¯", "ä¸ªäººä¿¡æ¯"]:
            continue
        clean_line = line.replace(" ", "")
        if re.match(r'^[\u4e00-\u9fa5]{2,4}$', clean_line):
            return clean_line
    
    return "æœªçŸ¥"


def _extract_email_fallback(text: str) -> str:
    """æå–é‚®ç®±ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', text)
    return match.group(0) if match else ""


def _extract_phone_fallback(text: str) -> str:
    """æå–æ‰‹æœºå·ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    match = re.search(r'1[3-9]\d{9}', text)
    return match.group(0) if match else ""


def _extract_location_fallback(text: str) -> str:
    """æå–æ‰€åœ¨åœ°ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    cities = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·", "æˆéƒ½", "æ­¦æ±‰", "å—äº¬", "è¥¿å®‰", "é‡åº†"]
    for city in cities:
        if city in text:
            return city + "å¸‚"
    return ""


def _extract_target_position_fallback(text: str, lines: List[str]) -> str:
    """æå–ç›®æ ‡å²—ä½ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    position_patterns = [
        r'æ±‚èŒæ„å‘[ï¼š:]\s*([^\n\r,ï¼Œã€;ï¼›]{2,25})',
        r'åº”è˜[å²—ä½èŒä½]*[ï¼š:]\s*([^\n\r,ï¼Œã€;ï¼›]{2,25})',
        r'ç›®æ ‡[å²—ä½èŒä½]*[ï¼š:]\s*([^\n\r,ï¼Œã€;ï¼›]{2,25})',
        r'æœŸæœ›[å²—ä½èŒä½]*[ï¼š:]\s*([^\n\r,ï¼Œã€;ï¼›]{2,25})',
    ]
    
    for pattern in position_patterns:
        match = re.search(pattern, text)
        if match:
            position = match.group(1).strip()
            position = re.sub(r'[\s\-â€”â€“()ï¼ˆï¼‰\[\]ã€ã€‘]+$', '', position)
            if position and 2 <= len(position) <= 25:
                return position
    
    return ""


def _extract_education_fallback(text: str) -> List[EducationItem]:
    """æå–æ•™è‚²èƒŒæ™¯ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    items = []
    education_section = _extract_section(text, ["æ•™è‚²èƒŒæ™¯", "æ•™è‚²ç»å†", "å­¦å†"])
    if not education_section:
        return items
    
    lines = education_section.split('\n')
    for line in lines:
        if re.search(r'\d{4}', line):
            school_match = re.search(r'[\u4e00-\u9fa5]{2,20}(å¤§å­¦|å­¦é™¢)', line)
            if school_match:
                items.append(EducationItem(
                    school=school_match.group(0),
                    major="",
                    degree="",
                    start_date="",
                    end_date=""
                ))
    
    return items


def _extract_experience_fallback(text: str) -> List[ExperienceItem]:
    """æå–å·¥ä½œç»å†ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    items = []
    experience_section = _extract_section(text, ["å·¥ä½œç»å†", "å·¥ä½œç»éªŒ", "ä»»èŒç»å†"])
    if not experience_section:
        return items
    
    lines = experience_section.split('\n')
    for line in lines:
        if re.search(r'\d{4}', line):
            company_match = re.search(r'[\u4e00-\u9fa5]{2,20}(å…¬å¸|ç§‘æŠ€|é›†å›¢)', line)
            if company_match:
                items.append(ExperienceItem(
                    company=company_match.group(0),
                    position="",
                    start_date="",
                    end_date="",
                    responsibilities=[]
        ))
    
    return items


def _extract_skills_fallback(text: str) -> List[str]:
    """æå–æŠ€èƒ½åˆ—è¡¨ï¼ˆå…œåº•æ–¹æ³•ï¼‰."""
    skills_section = _extract_section(text, ["æŠ€èƒ½", "ä¸“ä¸šæŠ€èƒ½", "æŠ€æœ¯æ ˆ"])
    if not skills_section:
        return []
    
    skills = re.split(r'[,ï¼Œã€\s]+', skills_section)
    return [s.strip() for s in skills if s.strip() and len(s.strip()) > 1]


def _extract_section(text: str, keywords: List[str]) -> str:
    """æå–ç‰¹å®šç« èŠ‚çš„å†…å®¹."""
    lines = text.split('\n')
    section_lines = []
    in_section = False
    
    for line in lines:
        if any(keyword in line for keyword in keywords):
            in_section = True
            continue
        
        if in_section:
            if len(line.strip()) < 10 and any(kw in line for kw in ["èƒŒæ™¯", "ç»å†", "ç»éªŒ", "èƒ½åŠ›", "è¯ä¹¦"]):
                break
            section_lines.append(line)
    
    return '\n'.join(section_lines)
