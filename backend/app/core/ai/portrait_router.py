"""
ç”»åƒä¸“ç”¨æ¨¡å‹è·¯ç”±å™¨ - V5 ä¸‰æ¨¡å‹åˆ†å±‚è°ƒç”¨

è·¯ç”±ç­–ç•¥ï¼ˆV5 æ›´æ–°ï¼‰ï¼š
1. æ·±åº¦åˆ†æï¼ˆProï¼‰: Qwen2.5-32B-Instruct - é»˜è®¤æ¨¡å‹ï¼Œæ‰€æœ‰ AI åˆ†æä¼˜å…ˆä½¿ç”¨
2. ä¸“å®¶åˆ†æï¼ˆExpertï¼‰: DeepSeek-R1-0528 - é‡è¦å€™é€‰äºº/æ·±åº¦æ´å¯Ÿï¼Œæ‰‹åŠ¨åˆ‡æ¢
3. å…œåº•ï¼ˆNormalï¼‰: Qwen2.5-7B-Instruct - Pro å¤±è´¥æ—¶è‡ªåŠ¨é™çº§

Fallback ç­–ç•¥ï¼š
Pro(32B) å¤±è´¥ â†’ Normal(7B) â†’ ç¡…åŸºæµåŠ¨ Qwen3-8B
"""

import logging
from typing import Any, Dict, List, Optional

from .ai_client import AIClientError, post_chat, parse_json_safely
from .modelscope_client import (
    ModelLevel, ModelScopeError, 
    call_modelscope, is_modelscope_available, get_model_info,
    get_modelscope_status, check_api_key_expiry
)
from .position_level import (
    PositionLevel, detect_position_level,
    get_level_display_name, get_level_description
)

logger = logging.getLogger(__name__)


def determine_analysis_level(
    position: Optional[str] = None,
    force_level: Optional[str] = None,
    resume_data: Optional[Dict[str, Any]] = None,
    competency_scores: Optional[Dict[str, int]] = None,
) -> str:
    """
    ç¡®å®šåˆ†æçº§åˆ«.
    
    ç°åœ¨é»˜è®¤ç›´æ¥ä½¿ç”¨ DeepSeek (pro)ï¼Œä»…åœ¨å¼ºåˆ¶æŒ‡å®š expert æ—¶ä»ä½¿ç”¨ expert æµç¨‹ã€‚
    """
    # å¼ºåˆ¶æŒ‡å®šçº§åˆ«ï¼ˆåªæ¥å— pro æˆ– expertï¼‰
    if force_level and force_level in ("pro", "expert"):
        logger.info(f"ğŸ¯ ä½¿ç”¨åˆ†æçº§åˆ«: {force_level}")
        return force_level
    
    # é»˜è®¤ä½¿ç”¨ proï¼ˆç°å·²æ˜ å°„åˆ° DeepSeekï¼‰
    logger.info("ğŸ“Š ä½¿ç”¨é»˜è®¤åˆ†æçº§åˆ«: pro (DeepSeek)")
    return "pro"


# ä¿ç•™æ—§å‡½æ•°åä»¥ä¿æŒå…¼å®¹æ€§
def should_use_pro_level(
    position: str,
    force_pro: bool = False,
    competency_scores: Optional[Dict[str, int]] = None,
) -> bool:
    """å…¼å®¹æ—§æ¥å£ - åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ Pro çº§."""
    level = determine_analysis_level(
        position=position,
        force_level="pro" if force_pro else None,
        competency_scores=competency_scores,
    )
    return level in ("pro", "expert")


async def call_portrait_model(
    messages: List[Dict[str, Any]],
    level: str = "normal",
    max_tokens: int = 1536,
    temperature: float = 0.3,
) -> Dict[str, Any]:
    """
    è°ƒç”¨ç”»åƒä¸“ç”¨æ¨¡å‹.
    
    è·¯ç”±é€»è¾‘ï¼š
    1. ä¼˜å…ˆä½¿ç”¨ ModelScopeï¼ˆå¦‚æœé…ç½®äº† API Keyï¼‰
    2. ModelScope å¤±è´¥æ—¶ï¼Œfallback åˆ°ç¡…åŸºæµåŠ¨
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        level: æ¨¡å‹çº§åˆ« ("normal" / "pro" / "expert")
        max_tokens: æœ€å¤§è¾“å‡º token
        temperature: æ¸©åº¦å‚æ•°
        
    Returns:
        API å“åº”å­—å…¸
    """
    # è½¬æ¢ level å­—ç¬¦ä¸²ä¸ºæšä¸¾ - V5: é»˜è®¤ä½¿ç”¨ PRO
    model_level = {
        "normal": ModelLevel.NORMAL,  # å…œåº•
        "pro": ModelLevel.PRO,        # é»˜è®¤
        "expert": ModelLevel.EXPERT,  # ä¸“å®¶çº§
    }.get(level, ModelLevel.PRO)  # V5: é»˜è®¤ PRO è€Œé NORMAL
    
    # ä¼˜å…ˆå°è¯• ModelScope
    if is_modelscope_available():
        try:
            print(f"ğŸ¯ ä½¿ç”¨ ModelScope ç”»åƒæ¨¡å‹ (level={level})")
            logger.info(f"ğŸ¯ ä½¿ç”¨ ModelScope ç”»åƒæ¨¡å‹ (level={level})")
            result = await call_modelscope(
                messages=messages,
                level=model_level,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            print(f"âœ… ModelScope è°ƒç”¨æˆåŠŸ model={result.get('model', 'unknown')}")
            return result
        except ModelScopeError as e:
            print(f"âš ï¸ ModelScope è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç¡…åŸºæµåŠ¨: {e}")
            logger.warning(f"âš ï¸ ModelScope è°ƒç”¨å¤±è´¥ï¼Œåˆ‡æ¢åˆ°ç¡…åŸºæµåŠ¨: {e}")
    else:
        print("ğŸ“Œ ModelScope æœªé…ç½®ï¼Œä½¿ç”¨ç¡…åŸºæµåŠ¨")
        logger.info("ğŸ“Œ ModelScope æœªé…ç½®ï¼Œä½¿ç”¨ç¡…åŸºæµåŠ¨")
    
    # Fallback åˆ°ç¡…åŸºæµåŠ¨
    try:
        result = await post_chat(
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        result["level"] = "fallback"
        return result
    except AIClientError as e:
        logger.error(f"âŒ æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        raise


async def generate_portrait(
    payload: Dict[str, Any],
    level: str = "pro",  # V5: é»˜è®¤ä½¿ç”¨ pro
    use_expert_summary: bool = False,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆå€™é€‰äººç”»åƒ - V5 ç‰ˆæœ¬.
    
    Args:
        payload: ç”»åƒç”Ÿæˆå‚æ•°
        level: åˆ†æçº§åˆ« (pro/expert)ï¼Œé»˜è®¤ pro
        use_expert_summary: æ˜¯å¦ä½¿ç”¨ä¸“å®¶çº§ç»¼åˆåˆ†æï¼ˆäºŒé˜¶æ®µç”Ÿæˆï¼‰
        
    Returns:
        ç”»åƒç»“æœå­—å…¸
    """
    from .prompt_builder import build_interpretation_prompt
    
    # V5: ç¡®ä¿ level æœ‰æ•ˆï¼Œé»˜è®¤ä½¿ç”¨ pro
    if level not in ("pro", "expert"):
        level = "pro"
    
    # æ„å»ºæç¤ºè¯ï¼ˆä¼ å…¥ level ä»¥é€‰æ‹©å¯¹åº”çš„ System Promptï¼‰
    messages = build_interpretation_prompt(payload, level=level)
    
    # ä½¿ç”¨æŒ‡å®šçº§åˆ«çš„æ¨¡å‹
    logger.info(f"ğŸ“Š ç”Ÿæˆç”»åƒ (level={level})")
    print(f"ğŸ“Š ç”Ÿæˆç”»åƒ (level={level})")
    result = await call_portrait_model(
        messages=messages,
        level=level,
        max_tokens=2048,  # V5: å¢åŠ  token é™åˆ¶ä»¥æ”¯æŒæ›´è¯¦ç»†çš„è¾“å‡º
        temperature=0.3,
    )
    
    # è§£æ JSON ç»“æœ
    content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
    parsed = parse_json_safely(content)
    
    # æ·»åŠ æ¨¡å‹ä¿¡æ¯
    parsed["_model"] = result.get("model", "unknown")
    parsed["_level"] = result.get("level", level)
    
    # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœå¯ç”¨ä¸“å®¶çº§ç»¼åˆåˆ†æï¼Œä½¿ç”¨ DeepSeek-R1 å¢å¼º
    if use_expert_summary:
        logger.info("ğŸ§  ç¬¬äºŒé˜¶æ®µï¼šå¯ç”¨ä¸“å®¶çº§ç»¼åˆåˆ†æ")
        scores = payload.get("scores", {})
        job_family = payload.get("job_family", "é€šç”¨")
        position_keywords = payload.get("position_keywords", [])
        target_position = position_keywords[0] if position_keywords else "é€šç”¨å²—ä½"
        
        expert_result = await generate_expert_summary(
            basic_portrait=parsed,
            scores=scores,
            target_position=target_position,
            job_family=job_family,
        )
        
        # åˆå¹¶ä¸“å®¶åˆ†æç»“æœ
        if expert_result.get("expert_summary"):
            # ç”¨ä¸“å®¶çº§ç»¼åˆåˆ†ææ›¿æ¢åŸæœ‰çš„ summary_points
            parsed["summary_points"] = expert_result.get("expert_summary", [])
            parsed["hiring_recommendation"] = expert_result.get("hiring_recommendation", "")
            parsed["interview_focus"] = expert_result.get("interview_focus", [])
            parsed["_expert_model"] = expert_result.get("_model", "unknown")
            logger.info("âœ… ä¸“å®¶çº§ç»¼åˆåˆ†æå·²åˆå¹¶åˆ°ç”»åƒ")
        else:
            logger.warning("âš ï¸ ä¸“å®¶çº§ç»¼åˆåˆ†æç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€ç”»åƒ")
    
    return parsed


async def generate_expert_summary(
    basic_portrait: Dict[str, Any],
    scores: Dict[str, Any],
    target_position: str,
    job_family: str = "é€šç”¨",
) -> Dict[str, Any]:
    """
    äºŒé˜¶æ®µç”Ÿæˆï¼šä½¿ç”¨ DeepSeek-R1 ç”Ÿæˆæ›´ç²¾å‡†çš„ç»¼åˆåˆ†æ.
    
    åœ¨ã€Œä¸“å®¶åˆ†æã€æ¨¡å¼ä¸‹ï¼Œç¬¬ä¸€é˜¶æ®µç”¨ Qwen ç”ŸæˆåŸºç¡€ç”»åƒï¼Œ
    ç¬¬äºŒé˜¶æ®µç”¨ DeepSeek-R1 ç”Ÿæˆé«˜è´¨é‡çš„ç»¼åˆåˆ†æã€‚
    
    Args:
        basic_portrait: ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„åŸºç¡€ç”»åƒ
        scores: æµ‹è¯„åˆ†æ•°
        target_position: ç›®æ ‡å²—ä½
        job_family: å²—ä½æ—
        
    Returns:
        åŒ…å« expert_summary å’Œ expert_insights çš„å­—å…¸
    """
    import json
    
    # æå–åŸºç¡€ç”»åƒä¸­çš„å…³é”®ä¿¡æ¯
    strengths = basic_portrait.get("strengths", [])
    risks = basic_portrait.get("risks", [])
    competencies = basic_portrait.get("competencies", [])
    personality = basic_portrait.get("personality_dimensions", [])
    
    system_prompt = """ä½ æ˜¯ä¸€åèµ„æ·±äººæ‰æµ‹è¯„ä¸“å®¶ï¼Œæ“…é•¿ç»¼åˆåˆ†æå€™é€‰äººç”»åƒå¹¶ç»™å‡ºç²¾å‡†ã€æœ‰æ´å¯ŸåŠ›çš„æ€»ç»“ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç»¼åˆåˆ†æå€™é€‰äººçš„æµ‹è¯„æ•°æ®ã€ä¼˜åŠ¿å’Œé£é™©ç‚¹
2. ç»™å‡º 3 æ¡ç²¾å‡†ã€æœ‰æ·±åº¦çš„ç»¼åˆåˆ†æè§‚ç‚¹
3. æ¯æ¡è§‚ç‚¹å¿…é¡»æœ‰å…·ä½“çš„æ•°æ®æ”¯æ’‘å’Œè¡Œä¸ºæ¨æ–­

è¾“å‡ºè¦æ±‚ï¼š
- ä¸è¦ç®€å•å¤è¿°å·²æœ‰ä¿¡æ¯ï¼Œè¦æœ‰äºŒé˜¶æ¨æ–­
- æ¯æ¡è§‚ç‚¹ 80-120 å­—ï¼Œä¿¡æ¯å¯†åº¦é«˜
- è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé¿å…ç©ºæ´çš„å½¢å®¹è¯
- ç»“åˆå²—ä½éœ€æ±‚åˆ†æåŒ¹é…åº¦"""

    user_prompt = f"""è¯·å¯¹ä»¥ä¸‹å€™é€‰äººè¿›è¡Œæ·±åº¦ç»¼åˆåˆ†æï¼š

ã€ç›®æ ‡å²—ä½ã€‘{target_position}ï¼ˆ{job_family}ï¼‰

ã€æµ‹è¯„åˆ†æ•°ã€‘
{json.dumps(scores, ensure_ascii=False, indent=2)}

ã€å·²è¯†åˆ«ä¼˜åŠ¿ã€‘
{json.dumps(strengths, ensure_ascii=False)}

ã€å·²è¯†åˆ«é£é™©ã€‘
{json.dumps(risks, ensure_ascii=False)}

ã€èƒœä»»åŠ›è¯„ä¼°ã€‘
{json.dumps(competencies, ensure_ascii=False)}

ã€äººæ ¼ç‰¹å¾ã€‘
{json.dumps(personality, ensure_ascii=False)}

è¯·è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "expert_summary": [
    "ç¬¬ä¸€æ¡ç»¼åˆåˆ†æè§‚ç‚¹ï¼ˆ80-120å­—ï¼‰",
    "ç¬¬äºŒæ¡ç»¼åˆåˆ†æè§‚ç‚¹ï¼ˆ80-120å­—ï¼‰",
    "ç¬¬ä¸‰æ¡ç»¼åˆåˆ†æè§‚ç‚¹ï¼ˆ80-120å­—ï¼‰"
  ],
  "hiring_recommendation": "æ˜¯å¦å»ºè®®å½•ç”¨åŠç†ç”±ï¼ˆ50å­—ä»¥å†…ï¼‰",
  "interview_focus": ["é¢è¯•é‡ç‚¹å…³æ³¨é—®é¢˜1", "é¢è¯•é‡ç‚¹å…³æ³¨é—®é¢˜2", "é¢è¯•é‡ç‚¹å…³æ³¨é—®é¢˜3"]
}}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    
    try:
        logger.info("ğŸ§  äºŒé˜¶æ®µç”Ÿæˆï¼šè°ƒç”¨ DeepSeek-R1 ç”Ÿæˆä¸“å®¶çº§ç»¼åˆåˆ†æ")
        result = await call_portrait_model(
            messages=messages,
            level="expert",
            max_tokens=1024,
            temperature=0.4,
        )
        
        content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        parsed = parse_json_safely(content)
        
        parsed["_model"] = result.get("model", "unknown")
        parsed["_level"] = "expert"
        
        logger.info("âœ… ä¸“å®¶çº§ç»¼åˆåˆ†æç”ŸæˆæˆåŠŸ")
        return parsed
        
    except Exception as e:
        logger.error(f"âŒ ä¸“å®¶çº§ç»¼åˆåˆ†æç”Ÿæˆå¤±è´¥: {e}")
        return {
            "expert_summary": [],
            "hiring_recommendation": "",
            "interview_focus": [],
            "_error": str(e),
        }


async def generate_expert_analysis(
    summary_json: Dict[str, Any],
    scores: Dict[str, Any],
    job_family: str,
    target_position: str,
) -> Dict[str, Any]:
    """
    ç”Ÿæˆä¸“å®¶çº§æ·±åº¦åˆ†æ.
    
    ä½¿ç”¨ DeepSeek-R1 å¯¹å·²æœ‰çš„ç”»åƒæ‘˜è¦è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œ
    è¾“å‡º 3 æ¡æ·±åº¦æ´å¯Ÿæˆ–é¢è¯•è¿½é—®å»ºè®®ã€‚
    
    Args:
        summary_json: 7B/32B ç”Ÿæˆçš„ç»“æ„åŒ–æ‘˜è¦
        scores: æµ‹è¯„åˆ†æ•°
        job_family: å²—ä½æ—
        target_position: ç›®æ ‡å²—ä½
        
    Returns:
        ä¸“å®¶åˆ†æç»“æœ
    """
    import json
    
    # æ„å»ºä¸“å®¶åˆ†ææç¤ºè¯
    system_prompt = """ä½ æ˜¯ä¸€åèµ„æ·±äººæ‰æµ‹è¯„ä¸“å®¶ï¼Œæ“…é•¿ä»å€™é€‰äººç”»åƒä¸­å‘ç°æ·±å±‚æ´å¯Ÿã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æå·²æœ‰çš„ç”»åƒæ‘˜è¦å’Œæµ‹è¯„æ•°æ®
2. æ‰¾å‡º HR å’Œç”¨äººç»ç†å¯èƒ½å¿½ç•¥çš„å…³é”®ç‚¹
3. ç»™å‡º 3 æ¡æ·±åº¦æ´å¯Ÿæˆ–é¢è¯•è¿½é—®å»ºè®®

è¾“å‡ºè¦æ±‚ï¼š
- æ¯æ¡æ´å¯Ÿå¿…é¡»æœ‰å…·ä½“çš„è¡Œä¸ºè¯æ®æ”¯æ’‘
- é¢è¯•è¿½é—®å»ºè®®è¦å…·ä½“ã€å¯æ“ä½œ
- è¯­è¨€ä¸“ä¸šä½†æ˜“æ‡‚"""

    user_prompt = f"""è¯·åˆ†æä»¥ä¸‹å€™é€‰äººç”»åƒï¼Œç»™å‡ºä¸“å®¶çº§æ´å¯Ÿï¼š

ã€ç›®æ ‡å²—ä½ã€‘{target_position}ï¼ˆ{job_family} å²—ä½æ—ï¼‰

ã€å·²æœ‰ç”»åƒæ‘˜è¦ã€‘
{json.dumps(summary_json, ensure_ascii=False, indent=2)}

ã€æµ‹è¯„åˆ†æ•°ã€‘
{json.dumps(scores, ensure_ascii=False, indent=2)}

è¯·è¾“å‡º JSON æ ¼å¼ï¼š
{{
  "expert_insights": [
    {{
      "type": "æ´å¯Ÿ/è¿½é—®/é£é™©é¢„è­¦",
      "title": "æ ‡é¢˜ï¼ˆ10å­—ä»¥å†…ï¼‰",
      "content": "å…·ä½“å†…å®¹ï¼ˆ50-100å­—ï¼‰",
      "evidence": "æ”¯æ’‘è¯æ®"
    }}
  ],
  "interview_questions": [
    "é¢è¯•è¿½é—®é—®é¢˜1",
    "é¢è¯•è¿½é—®é—®é¢˜2"
  ],
  "hiring_suggestion": "æ˜¯å¦å»ºè®®å½•ç”¨çš„æ€»ç»“ï¼ˆ30å­—ä»¥å†…ï¼‰"
}}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    
    try:
        result = await call_portrait_model(
            messages=messages,
            level="expert",
            max_tokens=1024,
            temperature=0.4,
        )
        
        content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
        parsed = parse_json_safely(content)
        
        parsed["_model"] = result.get("model", "unknown")
        parsed["_level"] = "expert"
        
        return parsed
        
    except Exception as e:
        logger.error(f"âŒ ä¸“å®¶åˆ†æç”Ÿæˆå¤±è´¥: {e}")
        return {
            "expert_insights": [],
            "interview_questions": [],
            "hiring_suggestion": "åˆ†æç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•",
            "_error": str(e),
        }


def get_router_status() -> Dict[str, Any]:
    """è·å–è·¯ç”±å™¨çŠ¶æ€ä¿¡æ¯."""
    modelscope_status = get_modelscope_status()
    api_key_status = check_api_key_expiry()
    
    models = []
    if modelscope_status["available"]:
        for level in ModelLevel:
            info = get_model_info(level)
            info["available"] = True
            models.append(info)
    
    return {
        "modelscope_available": modelscope_status["available"],
        "api_key_status": api_key_status,
        "models": models,
        "fallback_available": True,  # ç¡…åŸºæµåŠ¨æ€»æ˜¯å¯ç”¨çš„ï¼ˆå‡è®¾å·²é…ç½®ï¼‰
        "routing_strategy": "ModelScope â†’ SiliconFlow â†’ GLM",
    }

